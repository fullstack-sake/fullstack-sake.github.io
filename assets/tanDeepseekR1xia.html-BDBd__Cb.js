import{_ as r,r as n,o as a,c as s,d as o,e,a as p,b as t}from"./app-WuGmklff.js";const l="/assets/GiNuY9jWQAAJa-8-B3d_AWkw.jpg",d={},i=t('<div class="custom-container tip"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="9"></circle><path d="M12 8h.01"></path><path d="M11 12h1v4h1"></path></g></svg><p class="custom-container-title">TIP</p><p>论文链接：https://arxiv.org/abs/2501.12948</p></div><hr><h2 id="前言" tabindex="-1"><a class="header-anchor" href="#前言"><span>前言</span></a></h2>',3),h={href:"https://sakee.cn/blogs/jishu/tandeepseekr1.html",target:"_blank",rel:"noopener noreferrer"},g=t('<p>但是原论文分享的训练方法和细节并不多，复现难度还是挺大的。在许多传统观点中，我们认为训练一个具备强大推理能力的语言模型，必须对每一步推导过程进行精细的监督。但<code>Deepseek-R1</code>就是个例外。由此，我们继续深挖<code>Deepseek-R1</code>，从不同的角度来理解是如何奖励信号是如何激发推理能力的。</p><blockquote><p><em>注：这里的<code>Deepseek-R1</code>指<code>Deepseek-R1</code>系列，全文使用<code>R1</code>指代<code>R1</code>模型</em></p></blockquote><h2 id="一、从问题导向到结果导向" tabindex="-1"><a class="header-anchor" href="#一、从问题导向到结果导向"><span>一、从问题导向到结果导向</span></a></h2><p>在很多领域，我们习惯于“事事必查”，特别是在数学问题这种需要严密逻辑推导的任务中。目前大多数<code>benchmark</code>仅关注最终答案，对<code>solution</code>并没有严格要求，现有的改良思路是加入对<code>COT</code>的评判。正因为<code>COT</code>在提升模型推理能力上起到了关键作用，所以大部分模型的训练都会通过大量思考过程引导模型形成<code>COT</code>的范式。</p><p>这样也会出现两个问题：</p><blockquote><p>1.<strong>高昂的数据标注成本</strong>：需要大量人力对每个细节进行精细标注。</p><p>2.<strong>灵活性降低</strong>：过分约束中间过程可能让模型难以自主探索新策略。</p></blockquote><p>然而不同于前面，<code>R1-Zero</code>的设计核心是其实是将数学推理任务转化为一个仅依赖最终结果的<code>End-to-End</code>学习问题。</p><p>简单来说，我们不要求模型是不是按照人类偏好思考，是不是在每个中间步骤都完全正确，只要最终答案正确，然后包裹在<code>\\boxed{Answer}</code>，我们就给予正向奖励。</p><h2 id="二、-aha-moment-的再度解读" tabindex="-1"><a class="header-anchor" href="#二、-aha-moment-的再度解读"><span>二、“Aha Moment”的再度解读</span></a></h2><p>在<code>R1-zero</code>的纯<code>RL</code>训练过程中，模型有时会突然发现一种更高效的推理路径，这就是所谓的“顿悟”（<code>Aha Moment</code>）。</p><p>我认为这种现象并非依赖于提前教授某种推理技巧，而是依靠大量的探索得出来的：</p><blockquote><p>1.一方面是<code>GRPO</code>的<strong>在线采样</strong>机制，在每次训练迭代中，模型直接利用当前策略生成候选输出（例如一段<code>COT</code>或答案）。这多种可能的输出，既包括正确的也可能是错误的。</p></blockquote><blockquote><p>2.另一方面是<code>GRPO</code>的<strong>拒绝采样</strong>机制，它在对采样得到的候选输出进行筛选，丢弃低质量的样本。</p></blockquote><p><img src="'+l+'" alt="GRPO"></p><p><strong>在不断的采样后，模型发现候选输出中思考多次的输出奖励会更好。于是他学会了分配更多的思维时间来解决问题。（三思而后行）</strong></p><p>我们没有一个简单明确的数值或标准来衡量模型的<strong>推理深度</strong>。但是我们发现，当模型进入<code>“Aha Moment”</code>时，它往往会生成更多的推理<code>token</code>，这反映出模型在“<strong>花费</strong>”更多计算来进行复杂推理。就像是人的思考过程，有时候会陷入思考，有时候会突然有灵感。</p><p><strong>然而，这样就会造成一个问题：<code>Overthinking</code>（简单问题也会尝试大量的思考以及不断重复的确认自己的答案）</strong></p><h2 id="三、sft-与-rl" tabindex="-1"><a class="header-anchor" href="#三、sft-与-rl"><span>三、SFT 与 RL</span></a></h2><h3 id="_3-1-为什么-r1-zero-选择先进行-rl-而不-sft" tabindex="-1"><a class="header-anchor" href="#_3-1-为什么-r1-zero-选择先进行-rl-而不-sft"><span>3.1 为什么 R1-zero 选择先进行 RL 而不 SFT？</span></a></h3><p>三个方面：</p><blockquote><p>1.直接从<code>Base</code>模型出发，利用<code>RL</code>进行探索，保留<code>Base</code>模型的原始能力。</p></blockquote><blockquote><p>2.高质量的<code>Long-CoT</code>数据较为稀缺，而从现有数据中硬性蒸馏出的<code>CoT</code>可能会限制模型能力（指<s>OpenAI</s>控诉的蒸馏🤐）。</p></blockquote><blockquote><p>3.<code>R1-Zero</code>主要关注在数学问题上取得高分，因此只需要对最终答案进行奖励，而非每个推理步骤都做约束。</p></blockquote><h3 id="_3-2为什么最终的-r1-模型仍然需要-sft-和-preference-learning" tabindex="-1"><a class="header-anchor" href="#_3-2为什么最终的-r1-模型仍然需要-sft-和-preference-learning"><span>3.2为什么最终的 R1 模型仍然需要 SFT 和 Preference Learning？</span></a></h3><p>在<code>R1-Zero</code>训练基础上，<code>R1</code>模型进一步引入了<code>SFT</code>和偏好学习，主要目的在于：</p><blockquote><p>1.引入人类先验，提高可读性，帮助模型控制输出格式，使得生成的推理过程更符合人类思考逻辑。</p></blockquote><blockquote><p>2.不仅限于数学推理，通用数据（如代码、谜题、摘要等）能够使模型推理能力在其他领域得到泛化。</p></blockquote><p>简单来说就是：</p><p><strong><code>R1-Zero</code>是一个“原始智慧”的模型，而<code>R1</code>则像是保留了推理智慧的人类专家。</strong></p><h2 id="四、奖励驱动以及过程监督" tabindex="-1"><a class="header-anchor" href="#四、奖励驱动以及过程监督"><span>四、奖励驱动以及过程监督</span></a></h2><p>或许你会疑问：<strong>奖励驱动能否完全替代过程监督？</strong></p><p><strong>结论是：</strong></p><p>在特定的数学问题上，仅要求最终答案的这中奖励确实足以驱动模型学习出复杂推理能力，过程监督在这一步骤是无效且可以去除的。</p><p>但在开放性任务中（如文本续写）或需要对推理过程进行精细控制的场景下，需要额外的指令微调或过程监督辅助。如果去除人类先验，可能最终只有AI模型能看懂AI模型的“黑箱推理”想表达什么意思了。</p><p><strong>期待的技术演进</strong>：跨任务奖励驱动、分层强化学习、记忆增强机制</p><h2 id="五、结语" tabindex="-1"><a class="header-anchor" href="#五、结语"><span>五、结语</span></a></h2><p><code>R1-Zero</code>的实践证实了<code>End-to-End RL</code>在封闭域推理任务中的潜力，纯<code>RL</code>机制成功激发出模型的“认知涌现”能力。</p><p>接下来的一个关键研究方向就是：<strong>如何将数学推理中习得的抽象逻辑框架高效迁移至蛋白质设计、金融建模等复杂领域。</strong></p><p>同时，<code>DeepSeek-R1</code>在<code>MCTS</code>（蒙特卡洛树搜索）和<code>PRM</code>（过程奖励模型）失败尝试也提供了负样本，这表明：<strong>语言模型推理与传统符号推理在搜索空间、价值估计等维度具有本质的差异。</strong></p>',39);function k(u,m){const c=n("ExternalLinkIcon");return a(),s("div",null,[i,o("p",null,[e("在"),o("a",h,[e("上一篇博客"),p(c)]),e("的导读部分，我们提出了多个关键问题，这些问题贯穿了整个技术设计的逻辑链条。")]),g])}const b=r(d,[["render",k],["__file","tanDeepseekR1xia.html.vue"]]),f=JSON.parse(`{"path":"/blogs/jishu/tanDeepseekR1xia.html","title":"DeepSeek-R1 技术解析（下）","lang":"zh-CN","frontmatter":{"title":"DeepSeek-R1 技术解析（下）","katex":true,"date":"2025/02/02","tags":["LLM"],"categories":["技术"],"layout":"GlobalLayout","description":"TIP论文链接：https://arxiv.org/abs/2501.12948 前言 在上一篇博客的导读部分，我们提出了多个关键问题，这些问题贯穿了整个技术设计的逻辑链条。 但是原论文分享的训练方法和细节并不多，复现难度还是挺大的。在许多传统观点中，我们认为训练一个具备强大推理能力的语言模型，必须对每一步推导过程进行精细的监督。但Deepseek-R...","head":[["meta",{"property":"og:url","content":"https://sakee.cn/blogs/jishu/tandeepseekr1xia.html"}],["meta",{"property":"og:site_name","content":"sake's blog"}],["meta",{"property":"og:title","content":"DeepSeek-R1 技术解析（下）"}],["meta",{"property":"og:description","content":"TIP论文链接：https://arxiv.org/abs/2501.12948 前言 在上一篇博客的导读部分，我们提出了多个关键问题，这些问题贯穿了整个技术设计的逻辑链条。 但是原论文分享的训练方法和细节并不多，复现难度还是挺大的。在许多传统观点中，我们认为训练一个具备强大推理能力的语言模型，必须对每一步推导过程进行精细的监督。但Deepseek-R..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"fullstacksake"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2025-02-01T16:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DeepSeek-R1 技术解析（下）\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-02-01T16:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"fullstacksake\\",\\"email\\":\\"fullstacksake@outlook.com\\"}]}"]]},"headers":[{"level":2,"title":"前言","slug":"前言","link":"#前言","children":[]},{"level":2,"title":"一、从问题导向到结果导向","slug":"一、从问题导向到结果导向","link":"#一、从问题导向到结果导向","children":[]},{"level":2,"title":"二、“Aha Moment”的再度解读","slug":"二、-aha-moment-的再度解读","link":"#二、-aha-moment-的再度解读","children":[]},{"level":2,"title":"三、SFT 与 RL","slug":"三、sft-与-rl","link":"#三、sft-与-rl","children":[{"level":3,"title":"3.1 为什么 R1-zero 选择先进行 RL 而不 SFT？","slug":"_3-1-为什么-r1-zero-选择先进行-rl-而不-sft","link":"#_3-1-为什么-r1-zero-选择先进行-rl-而不-sft","children":[]},{"level":3,"title":"3.2为什么最终的 R1 模型仍然需要 SFT 和 Preference Learning？","slug":"_3-2为什么最终的-r1-模型仍然需要-sft-和-preference-learning","link":"#_3-2为什么最终的-r1-模型仍然需要-sft-和-preference-learning","children":[]}]},{"level":2,"title":"四、奖励驱动以及过程监督","slug":"四、奖励驱动以及过程监督","link":"#四、奖励驱动以及过程监督","children":[]},{"level":2,"title":"五、结语","slug":"五、结语","link":"#五、结语","children":[]}],"git":{},"filePathRelative":"blogs/技术/探DeepseekR1下.md","autoDesc":true}`);export{b as comp,f as data};
